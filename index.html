<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We take a critical look at which datasets are best for visuo-motor pre-training, and find that standard vision datasets are surprisingly strong!">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>An Unbiased Look at Datasets for Visuo-Motor Pre-Training</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link rel="icon" href="https://www.ri.cmu.edu/app/uploads/2017/02/ri-placeholder-66x66.jpg" sizes="32x32" />
  <link rel="icon" href="https://www.ri.cmu.edu/app/uploads/2017/02/ri-placeholder-200x200.jpg" sizes="192x192" />

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
      
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">An Unbiased Look at Datasets for Visuo-Motor Pre-Training</h1>
          <div class="is-size-3">Published in the 7<sup>th</sup> Conference on Robot Learning</div>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sudeepdasari.github.io">Sudeep Dasari</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.mohansrirama.com">Mohan Kumar Srirama</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://unnat.github.io">Unnat Jain</a><sup>1,2 <b>*</b></sup>,
            </span>
            <span class="author-block">
              <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><sup>1 <b>*</b></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1 </sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>2 </sup>FAIR at Meta</span>
            <br>
            <span class="author-block"> <sup><b>*</b></sup> <font size="-0.5">Denotes Equal Advising</font></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- TODO Paper -->
              <span class="link-block">
                <a href="./resources/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- TODO Arxiv -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- TODO Twitter -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                </a>
              </span>
              <!-- TODO Code -->
              <span class="link-block">
                <a href="https://github.com/SudeepDasari/data4robotics"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- TODO Models-->
              <span class="link-block">
                <a href="https://cmu.box.com/s/rrlrp5g6ynk03io4rj9uzf6nik5urfl6"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-eye"></i>
                  </span>
                  <span>Models</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="100%">
        <source src="./resources/teaser.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            This project conducts a <i>dataset centric</i> analysis of robotic pre-training. Our findings call into question some common wisdom in the field. We show that standard vision datasets (like ImageNet and Kinetics) are surprisingly competitive options for visuo-motor representation learning, and that the pre-training dataset's image distribution matters more than its size. Finally, we find that common simulation benchmarks give a misleading sense of progress, and that simple regularization strategies can dramatically improve real world policy learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation Methods</h2>
        <div class="content has-text-justified">
          <p>
            We pre-train representations using self-supervised learning algorithms (e.g., <a href="https://arxiv.org/abs/2111.06377">MAE</a> and <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>), on <b>different image datasets</b>. Representations are evaluated by fine-tuning them (via Behavior Cloning) to solve a suite of manipulation tasks, both in sim and on a real Franka robot (see below).
          </p>
        </div>

        <h2 class="title is-3">Real World Tasks</h2>

        <div class="content has-text-justified">
          <p>
            Videos of the 3 Real World tasks used in our evaluation played at 1X speed. Note how they range from easy (stacking) to hard (toasting), but they all include some degree of generalization (e.g. to new positions/objects/configurations). For in-depth visualizations, please refer to our <a href="https://drive.google.com/file/d/11gShQgnk3Ogf-cyFxoCrEimrERy94GiB/view?usp=share_link">results video.</a>
          </p>
          <div class="container  has-text-centered">
            <table>

              <tr>
                <td>
                  <div class="item item-stacking">
                    <video poster="" id="stacking" autoplay controls muted loop playsinline height="100%">
                    <source src="./resources/videos/stacking.mp4"
                            type="video/mp4">
                    </video>
                    <p><b>Stacking</b></p> 
                  </div>
                </td>
                <td>
                  <div class="item item-pouring">
                    <video poster="" id="pouring" autoplay controls muted loop playsinline height="100%">
                      <source src="./resources/videos/pouring.mp4"
                              type="video/mp4">
                    </video>
                    <p><b>Pouring</b></p> 
                  </div>
                </td>
                <td>
                  <div class="item item-toasting">
                    <video poster="" id="toasting" autoplay controls muted loop playsinline height="100%">
                      <source src="./resources/videos/toasting.mp4"
                              type="video/mp4">
                    </video>
                    <p><b>Toasting</b></p> 
                  </div>
                </td>
              </tr>
            </table>
            </div>
          </div>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Results</h2>
        <div class="content has-text-justified">
          <p> We pre-train representations on 5 unique datasets - <a href="https://www.image-net.org">ImageNet</a>, <a href="https://ego4d-data.org">Ego4D</a>, <a href="https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/">100 Days of Hands</a> (DoH), <a href="https://www.deepmind.com/open-source/kinetics">Kinetics</a>, and <a href="http://robo-net.github.io">RoboNet</a> - and evaluate their suitable for robotic representation learning. </p>
        </div>

        <h2 class="title is-4">Image Distribution More Important than Content</h2>
        <div class="content has-text-justified">
          
          <table style="width: 100%;">
            <tr>
                <td style="width: 50%;">
                    <div class="item item-stacking">
                      <p>
                        Our experiments reveal that ImageNet, Kinetics, and DoH (blue) representations all perform better than those trained on RoboNet or Ego4D (red) and prior SOTA baselines (gray). This is surprising, since both Ego4D and RoboNet seem like better matches to the test tasks -- e.g.,  RoboNet entirely contains images of robot interactions. These results strongly suggest that the pre-training image distribution is far more important than the images' content. <b>We combine these insights to create a final SOUP + DoH model that exceeds all prior baselines by 30!</b>
                      </p>
                    </div>
                </td>
                <td style="width: 50%;">
                    <div class="item item-pouring">
                        <img src="./resources/plots/tab_1_robot_plots.png" alt="Loading!" style="padding-top:0px;padding-bottom:0px;border-radius:15px">
                    </div>
                </td>
            </tr>
        </table>

        </div>

        <h2 class="title is-4"> Sim Evaluation Gives a False Sense of Progress </h2>
        <div class="content has-text-justified">
          <table style="width: 100%;">
            <tr>
                <td style="width: 50%;">
                    <div class="item item-stacking">
                      <p>
                        We consistently found that these (real world) results were not replicated in simulation! In fact, when the sim performance is plotted against real performance (across all models) it becomes clear that the two values are almost entirely uncorrelated: $R^2 = 32\%$. Even if you compare the two most similar sim and real tasks (RoboMimic's block-lift v.s., our stacking task) the correlation is still very low: $R^2 = 34\%$. This result is not necessarily surprising, since the sim2real gap in manipulation is well known. However, we feel it is important to explicitly do this analysis, since it's still very common practice in prior work to draw inferences about pre-trained representations using simulated benchmarks.
                      </p>
                    </div>
                </td>
                <td style="width: 50%;">
                    <div class="item item-pouring">
                        <img src="./resources/plots/sim2real.png" alt="Loading!" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                    </div>
                </td>
            </tr>
        </table>
        </div>

      </div>
    </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{dasari2023datasets,
      title={An Unbiased Look at Datasets for Visuo-Motor Pre-Training},
      author={Dasari, Sudeep and Srirama, Mohan Kumar and Jain, Unnat and Gupta, Abhinav},
      booktitle={Conference on Robot Learning},
      year={2023},
      organization={PMLR}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies template</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
